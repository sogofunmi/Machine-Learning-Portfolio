{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "## Build an Image Caption Generator System\n",
    "\n",
    "In this project, I will design and implement a deep learning model that learns to properly caption images. I will train the model using data gotten from the Flickr8k Dataset. Transfer learning will be used on the images so as to accurately extract the features. ResNet101 is the pre-trained network used. \n",
    "\n",
    "\n",
    "Pytorch will be used to build the model. \n",
    "\n",
    "### Implementation\n",
    "I started by importing the modules required for this project and loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from random import seed\n",
    "from random import choice\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "seed(10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we load in the captions data and view the first five rows of the dataframe\n",
    "#We display an example image from our dataset\n",
    "\n",
    "file_path = \"Flickr8k_Dataset\"\n",
    "df = pd.read_csv(\"Flickr8k_text/captions.txt\")\n",
    "\n",
    "train_size = len(df) - 200\n",
    "df, test_df = df.head(train_size), df.tail(1000)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_list = test_df[\"image\"].tolist()\n",
    "\n",
    "image = Image.open(os.path.join(file_path, df[\"image\"][0])).convert(\"RGB\")\n",
    "display(image)\n",
    "test_list = test_df[\"image\"].unique().tolist()\n",
    "len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define transforms for our data. Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(40),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Dataset\n",
    "Here I built a vocabuary which contains all the words in the captions data and I created a dictionary which maps out the words to an integer representing the number of times those words appear in the data. I also built a dataset which transforms each image in the data and retrieves each caption and adds it to the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function removes punctuation from the captions and returns a list of the lower-cased words\n",
    "def clean_captions(captions):\n",
    "    caption = [char for char in captions if char not in string.punctuation]\n",
    "    caption = \"\".join(caption)\n",
    "    caption = caption.split(\" \")\n",
    "    caption = [word.lower() for word in caption]\n",
    "    return caption\n",
    "\n",
    "#We define our vocabulary class \n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, frequency):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param frequency: Limit on the minimum occurence of each word. This removes irrelevant words from the vocabulary\n",
    "        \"\"\"\n",
    "            \n",
    "        self.int_to_vocab = {0:\"pad\", 1:\"startseq\", 2:\"endseq\", 3:\"unknown\"}\n",
    "        self.vocab_to_int = {\"pad\":0, \"startseq\":1, \"endseq\":2, \"unknown\":3}\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.int_to_vocab)\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        word_count = {}\n",
    "        index = 4\n",
    "        for caption in captions:\n",
    "            for word in clean_captions(caption):\n",
    "                if word not in word_count:\n",
    "                    word_count[word] = 1\n",
    "                else:\n",
    "                    word_count[word] += 1\n",
    "                if word_count[word] == self.frequency:\n",
    "                    self.vocab_to_int[word] = index\n",
    "                    self.int_to_vocab[index] = word\n",
    "                    index += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        tokens = clean_captions(caption)\n",
    "        return [\n",
    "            self.vocab_to_int[token] if token in self.vocab_to_int\n",
    "            else self.vocab_to_int[\"unknown\"] for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, df, transform, frequency=10):\n",
    "        \"\"\"\n",
    "        :param file_path: File path of our image data\n",
    "        :param df: Our captions dataframe\n",
    "        :param transform: For image transformations\n",
    "        :param frequency: Limit on the minimum occurence of each word.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "        self.vocab = Vocab(frequency)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param key: Index that enables us to retrieve each image and corresponding caption from our dataframe\n",
    "        \"\"\"\n",
    "        caption = self.captions[key]\n",
    "        image_id = self.images[key]\n",
    "        image = Image.open(os.path.join(self.file_path, image_id)).convert(\"RGB\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.vocab_to_int[\"startseq\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.vocab_to_int[\"endseq\"])\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    #This function pads the caption so all captions are of equal length\n",
    "    \n",
    "    def __init__(self, pad):\n",
    "        \"\"\"\n",
    "        param pad: To pad variable length tensors (int)\n",
    "        \"\"\"\n",
    "        self.pad = pad\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        image = [item[0].unsqueeze(0) for item in batch]\n",
    "        image = torch.cat(image, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = nn.utils.rnn.pad_sequence(targets, padding_value=self.pad)\n",
    "\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use SubsetRandomSampler to retirve data for our training and validation sets\n",
    "\n",
    "num_train = len(df)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(num_train * 0.8))\n",
    "np.random.shuffle(indices)\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "print(len(val_idx))\n",
    "print(len(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We define a dataloader which loads our data in batches\n",
    "\n",
    "def dataloader(file_path, df, transform, batch_size=64, pin_memory=True, data=\"train\"):\n",
    "    \"\"\"\n",
    "    param file_path: path to file\n",
    "    param df: dataframe consisting of the captions dataset\n",
    "    param transform: for transforming our image data\n",
    "    param batch_size: number of samples to be yielded\n",
    "    param pin_memory: setting to true enables faster daat transfer to CUDA-enabled GPUs\n",
    "    param data: for building our training and validation sets\n",
    "    \"\"\"\n",
    "    dataset = MyDataset(file_path, df, transform=transform)\n",
    "    pad = dataset.vocab.vocab_to_int[\"pad\"]\n",
    "    \n",
    "    if data == \"train\":\n",
    "        trainloader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=Collate(pad=pad), pin_memory=pin_memory, sampler=train_sampler)\n",
    "\n",
    "        return trainloader, dataset\n",
    "\n",
    "    else:\n",
    "        validloader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=Collate(pad=pad), pin_memory=pin_memory, sampler=valid_sampler)\n",
    "\n",
    "        return validloader\n",
    "\n",
    "def main(): \n",
    "    train_loader, dataset = dataloader(file_path, df, transform=transform, data=\"train\")\n",
    "    val_loader = dataloader(file_path, df, transform=transform, data=\"val\")\n",
    "    for images, captions in val_loader:\n",
    "        print(captions.shape)\n",
    "        print(images.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = models.resnet152(pretrained=True) \n",
    "for name, param in res_model.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "if train_on_gpu:\n",
    "    res_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size, model):\n",
    "        \"\"\"\n",
    "        :param embed_size: number of expected features\n",
    "        :param model: pre-trained model to be used for transfer learning\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        modules = list(model.children())[:-1]      # delete the last fc layer.\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(model.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.model(images) \n",
    "        features = features.reshape(features.size(0), -1) # 64 * 2048\n",
    "        features = self.bn(self.linear(features))\n",
    "        \n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param embed_size: number of expected features\n",
    "        :param hidden_size: number of features in the hidden layer \n",
    "        :param vocab_size: size of dictionary of embeddings\n",
    "        :param num_layers: number of recurrent layers\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        output = self.linear(hiddens)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, model):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.encoder = Encoder(embed_size, model)\n",
    "        self.decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        output = self.decoder(features, captions)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def captions(self, image, vocab, max_length=50):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param image: image to be captioned\n",
    "        :param vocab: the vocab list\n",
    "        :param max_length:\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(0)\n",
    "            states = None\n",
    "            for _ in range(max_length):\n",
    "                hidden, states = self.decoder.lstm(x, states)\n",
    "                output = self.decoder.linear(hidden.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                result.append(predicted.item())\n",
    "                x = self.decoder.embed(predicted).unsqueeze(0)\n",
    "                if vocab.int_to_vocab[predicted.item()] == \"endseq\":\n",
    "                    break\n",
    "                    \n",
    "            return [vocab.int_to_vocab[index] for index in result]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which un normalizes selected images and displays them \n",
    "\n",
    "def imshow(image_id):\n",
    "    image = test_transforms(Image.open(os.path.join(file_path, image_id)).convert(\"RGB\"))\n",
    "    c,w,h = image.shape\n",
    "    modified_img = np.reshape(image, (1,c,w,h))\n",
    "    title = \" \".join(model.captions(modified_img.cuda(), dataset.vocab))\n",
    "    \n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, dataset = dataloader(file_path, df, transform=train_transforms)\n",
    "valid_loader = dataloader(file_path, df, transform=test_transforms, data=\"val\")\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "val_loss_min = np.Inf\n",
    "model = CaptionModel(embed_size=512, hidden_size=512, vocab_size=len(dataset.vocab), num_layers=2, model=res_model).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-4)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for index, (images, captions) in enumerate(train_loader):\n",
    "        if train_on_gpu:\n",
    "            images, captions = images.cuda(), captions.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, captions[:-1])\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for index, (images, captions) in enumerate(valid_loader):\n",
    "                if train_on_gpu:\n",
    "                    images, captions = images.cuda(), captions.cuda()\n",
    "                output = model(images, captions[:-1])\n",
    "                loss = criterion(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "        train_loss = running_loss/len(train_loader)\n",
    "        valid_loss = val_loss/len(valid_loader)\n",
    "        \n",
    "        print(f\"\\nEpoch: {e+1}/{epochs}\",\n",
    "              f\"Training Loss: {train_loss}\",\n",
    "              f\"Validation Loss: {valid_loss}\")\n",
    "        \n",
    "        if valid_loss <= val_loss_min:\n",
    "            print(f\"Validation loss decreased ({val_loss_min} --> {valid_loss}). Saving model ...\")\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            val_loss_min = valid_loss\n",
    "        model.eval()\n",
    "        selection = choice(test_list)\n",
    "        imshow(selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
