{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "## Build a Music Genre Classification System\n",
    "\n",
    "In this project, I will design and implement a deep learning model that learns to properly classify music based on genres. I will train the model using data gotten from the GTZAN Dataset. \n",
    "\n",
    "Pytorch will be used to build the model. \n",
    "\n",
    "### Implementation\n",
    "I started by importing the modules required for this project and loading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/features_30_sec.csv\")\n",
    "df[\"label\"].unique()\n",
    "df[\"class_id\"] = pd.Categorical(df.label).codes\n",
    "classes = list(df[\"label\"].unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Data/genres_original\"\n",
    "\n",
    "sample_rate = 22050\n",
    "duration = 30 #Each audio is 30 seconds long\n",
    "samples_per_track = sample_rate * duration\n",
    "\n",
    "error_file = \"jazz.00054.wav\"\n",
    "\n",
    "def extract_mfcc_features(file_path, num_segments=10, n_mfcc=20, n_fft=2048, hop_length=512):\n",
    "    \n",
    "    \"\"\"\n",
    "    :param file_path: path to file\n",
    "    :param num_segments: number of segments sample tracks will be divided into\n",
    "    :param n_mfcc: number of coefficients to extract\n",
    "    :param n_fft: interval considered to apply the Fast Fourier transform (FFT)\n",
    "    :param hop_length: sliding window for the FFT\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": [],\n",
    "        }\n",
    "    samples_per_segment = int(samples_per_track / num_segments)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        folder = df[\"label\"].iloc[i]\n",
    "        filename = df['filename'].iloc[i]\n",
    "\n",
    "        path = f\"Data/genres_original/{folder}/{filename}\"\n",
    "    \n",
    "        if filename != error_file:\n",
    "            for num in range(num_segments):\n",
    "                signal_audio, sr = librosa.load(path, sr=sample_rate)\n",
    "                start_sample = samples_per_segment * num\n",
    "                finish_sample = start_sample + samples_per_segment\n",
    "                mfcc = librosa.feature.mfcc(signal_audio[start_sample:finish_sample], \n",
    "                                            sr=sr, n_mfcc=n_mfcc, \n",
    "                                            n_fft=n_fft, hop_length=hop_length)\n",
    "                mfcc = np.mean(mfcc.T, axis=0)\n",
    "                data[\"mfcc\"].append(mfcc.tolist())\n",
    "                data[\"labels\"].append(df[\"class_id\"].iloc[i])\n",
    "    \n",
    "    features = torch.tensor(data[\"mfcc\"])\n",
    "    labels = torch.tensor(data[\"labels\"])\n",
    "    \n",
    "    torch.save(features, \"data/features.pt\")\n",
    "    torch.save(labels, \"data/label.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_mfcc_features(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load our data and create the dataloaders needed for our training, testing, and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=torch.load(\"data/features.pt\")\n",
    "labels=torch.load(\"data/label.pt\")\n",
    "\n",
    "def get_dataset():\n",
    "    data=[]\n",
    "    label=[]\n",
    "    for i in range(len(features)):\n",
    "        data.append(features[i])\n",
    "        label.append(labels[i])\n",
    "    data = torch.stack(data)\n",
    "    label = torch.stack(label)\n",
    "    label = label.long()\n",
    "   \n",
    "    return TensorDataset(data, label)\n",
    "\n",
    "dataset = get_dataset()\n",
    "\n",
    "split_size = int(0.3 * len(dataset))\n",
    "train_size = len(dataset) - split_size\n",
    "\n",
    "val_size = int(0.5 * split_size)\n",
    "test_size = split_size - val_size\n",
    "\n",
    "train_ds, remaining_ds = random_split(dataset, [train_size, split_size])\n",
    "val_ds, test_ds = random_split(remaining_ds, [val_size, test_size])\n",
    "\n",
    "print(len(train_ds)) \n",
    "print(len(val_ds))\n",
    "print(len(test_ds))\n",
    "trainloader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "validloader = DataLoader(val_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "testloader = DataLoader(test_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = F.log_softmax(self.fc5(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-4)\n",
    "\n",
    "epochs = 100\n",
    "steps = 0\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for features, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(features)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        val_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for features, labels in validloader:\n",
    "                log_ps = model(features)\n",
    "                val_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(val_loss/len(validloader))\n",
    "        \n",
    "        running_loss = running_loss/len(trainloader)\n",
    "        val_loss = val_loss/len(validloader)\n",
    "        \n",
    "        print(f\"\\nEpoch: {e+1}/{epochs}\",\n",
    "              f\"Training Loss: {running_loss}\",\n",
    "              f\"Validation Loss: {val_loss}\",\n",
    "              f\"Validation Accuracy: {accuracy/len(validloader)}\")\n",
    "        \n",
    "        if val_loss <= val_loss_min:\n",
    "            print(f\"Validation loss decreased ({val_loss_min} --> {val_loss}). Saving model\")\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test accuracy of each genre and the test accuracy overall is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "for features, labels in testloader:\n",
    "    \n",
    "    batch_size = features.size(0)\n",
    "    output = model(features)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()\n",
    "    ps = torch.exp(output)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    \n",
    "    _, pred = torch.max(output, 1)    \n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    \n",
    "   \n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "    \n",
    "test_loss = test_loss/len(testloader.dataset)\n",
    "print(f\"Test Loss: {test_loss}\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print(f\"Test Accuracy of {classes[i]}: {round((100 * class_correct[i]) / class_total[i], 2)} ({int(np.sum(class_correct[i]))}/{int(np.sum(class_total[i]))})\")\n",
    "        \n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print(f\"\\nTest Accuracy (Overall): {(100. * np.sum(class_correct) / np.sum(class_total))} ({int(np.sum(class_correct))}/{int(np.sum(class_total))})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
